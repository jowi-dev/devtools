# Audit (Language-Agnostic Template)

## Purpose
Run comprehensive quality checks to ensure code meets project standards before committing or pushing.

This is your **pre-flight checklist** - catch issues before they reach code review or production.

## Philosophy

**Trust but verify:**
- You wrote good code
- The tools verify it meets standards
- Fix issues now, not later during review
- Automated checks are fast and consistent

**Quality gates:**
- Formatting (code looks consistent)
- Linting (code follows best practices)
- Type checking (types are correct, if applicable)
- Compilation (code builds successfully)
- Tests (code works as expected)

## When to Use This Skill

**Run audit:**
- âœ… Before committing code
- âœ… After implementing a feature
- âœ… Before creating a pull request
- âœ… After refactoring
- âœ… When you want confidence everything is clean

**Don't skip audit when:**
- âš ï¸ Making production changes
- âš ï¸ Creating pull requests
- âš ï¸ Finishing a feature
- âš ï¸ Unsure if everything is working

## Process

### 1. Run Formatter
**Purpose:** Ensure consistent code style

- Run auto-formatter on all changed files
- Formatter should enforce project style guide
- No manual formatting decisions
- Consistent indentation, spacing, line breaks

**Success:** All files pass formatting checks

### 2. Run Linter
**Purpose:** Catch common mistakes and enforce best practices

- Check for code smells
- Find potential bugs
- Enforce language idioms
- Flag unused code
- Suggest improvements

**Success:** No linting errors or warnings (unless explicitly allowed)

### 3. Check Types (if applicable)
**Purpose:** Verify type correctness

- Type-checked languages: Run type checker
- Dynamic languages with types: Run type hint checker
- Ensure type annotations are correct and complete

**Success:** No type errors

### 4. Compile/Build
**Purpose:** Ensure code can be built

- Compiled languages: Run compiler
- Check for syntax errors
- Verify dependencies are available
- Build with warnings as errors (if project requires)

**Success:** Clean compilation, zero warnings

### 5. Run Tests
**Purpose:** Verify behavior is correct

- Run full test suite
- All tests must pass
- Check test coverage (if project requires minimum)
- Verify no tests were accidentally skipped

**Success:** All tests green, expected coverage met

### 6. Report Results
**Summary of:**
- âœ… What passed
- âŒ What failed
- âš ï¸ Warnings or concerns
- ğŸ“Š Metrics (coverage, complexity, etc.)

## Success Criteria

- [ ] Code is formatted according to project standards
- [ ] No linting errors or warnings
- [ ] Types are correct (if applicable)
- [ ] Code compiles with zero warnings
- [ ] All tests pass
- [ ] Test coverage meets requirements (if applicable)
- [ ] Ready to commit or create PR

## Language-Specific Implementations

This template is implemented by:

- **Elixir**: `/audit-elixir`
  - `mix format --check-formatted`
  - `mix compile --warnings-as-errors`
  - `mix credo --strict`
  - `MIX_ENV=test mix test`

- **Python**: `/audit-python`
  - `black --check .`
  - `pylint src/`
  - `mypy src/`
  - `pytest`

- **TypeScript**: `/audit-typescript`
  - `prettier --check .`
  - `eslint .`
  - `tsc --noEmit`
  - `npm test`

- **Ruby**: `/audit-ruby`
  - `rubocop`
  - `bundle exec rspec`

## Example Output

```
ğŸ¯ Running audit...

âœ… Formatting
   All files formatted correctly

âœ… Linting
   0 errors, 0 warnings

âœ… Type Checking
   No type errors found

âœ… Compilation
   Build successful, 0 warnings

âœ… Tests
   42 tests, 42 passed, 0 failed
   Coverage: 94% (meets 90% requirement)

ğŸ‰ All checks passed! Ready to commit.
```

## Fixing Issues

### If formatting fails:
```
Run formatter: <language-specific command>
Then re-run audit
```

### If linting fails:
```
Fix each warning/error manually
Or apply auto-fixes if available
Then re-run audit
```

### If compilation fails:
```
Read error messages carefully
Fix syntax/import/type errors
Then re-run audit
```

### If tests fail:
```
Read test failure messages
Fix the bug or update the test
Then re-run audit
```

## Common Mistakes

âŒ **Skipping audit** - "I'll run it later" â†’ You forget
âŒ **Ignoring warnings** - Warnings become errors eventually
âŒ **Committing with failures** - Broken code shouldn't be committed
âŒ **Not reading error messages** - They tell you what's wrong
âŒ **Auto-fixing without understanding** - Know why it was wrong

## Quality Standards

### Zero tolerance:
- âŒ Syntax errors
- âŒ Type errors
- âŒ Failing tests
- âŒ Compilation warnings (if project requires)

### Fix before committing:
- âš ï¸ Linting warnings
- âš ï¸ Unused variables/imports
- âš ï¸ Low test coverage (if project requires minimum)

### Can be addressed later:
- ğŸ“ Complex refactoring suggestions
- ğŸ“ Performance optimizations
- ğŸ“ Additional test cases

## Integration with Workflow

### TDD Cycle
```
1. /tdd-red â†’ Write test â†’ audit (should fail)
2. /implement â†’ Fix code â†’ audit (should pass)
3. /commit-checkpoint â†’ Commit
```

### Before PR
```
1. Finish all features
2. /audit â†’ Fix any issues
3. Create PR
```

### Continuous Quality
```
Run audit frequently during development
Catch issues early when they're easier to fix
```

## CI/CD Integration

Many projects run these same checks in CI/CD:
- CI runs audit on every push
- PR requires all checks to pass
- Running audit locally saves time
- Catch issues before pushing

## The Quality Loop

```
Write code
    â†“
Run audit
    â†“
Issues? â†’ Fix them â†’ Run audit again
    â†“
All pass? â†’ Commit
```

**Don't skip the audit.** It's faster to fix issues now than during code review.
